{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e28eaa-10a3-4fa8-86d2-5618d389e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08cd0d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgzL59oMDnAHNV-96a94AaABAg</td>\n",
       "      <td>@Reverie-velzvet</td>\n",
       "      <td>UCJaSuHOdO58aWym_dnK8zMg</td>\n",
       "      <td>don't worry about staying in japan pewds cas t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-11T08:19:03Z</td>\n",
       "      <td>2025-04-11T08:19:03Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugw8bSwFjlEE2fqEW3x4AaABAg</td>\n",
       "      <td>@Reverie-velzvet</td>\n",
       "      <td>UCJaSuHOdO58aWym_dnK8zMg</td>\n",
       "      <td>9:15 I think it's the very first time pewds ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-11T08:00:23Z</td>\n",
       "      <td>2025-04-11T08:00:23Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugw4ZylvfRjfLe1XBSl4AaABAg</td>\n",
       "      <td>@yesjermplayzgamez511</td>\n",
       "      <td>UC89mwzaR6LHhk1pffPSBFdQ</td>\n",
       "      <td>As a new dad, and someone who used to watch yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-11T07:25:24Z</td>\n",
       "      <td>2025-04-11T07:25:24Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgxKwJLOpGXFfyFiuiJ4AaABAg</td>\n",
       "      <td>@RenuCacciatore75</td>\n",
       "      <td>UCYWEeTm05dumslGsMEAUErw</td>\n",
       "      <td>0:35 uuuu marimekko canvas bag i have one of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-10T17:00:39Z</td>\n",
       "      <td>2025-04-10T17:08:10Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgwIUQ2Z7KlcW3NlCW94AaABAg</td>\n",
       "      <td>@Alexooooo11</td>\n",
       "      <td>UCd4XgS8-vukv3F5JWx3eNPw</td>\n",
       "      <td>He look so calculated for his age</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-10T13:41:31Z</td>\n",
       "      <td>2025-04-10T13:41:31Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   comment_id                 author  \\\n",
       "0  UgzL59oMDnAHNV-96a94AaABAg       @Reverie-velzvet   \n",
       "1  Ugw8bSwFjlEE2fqEW3x4AaABAg       @Reverie-velzvet   \n",
       "2  Ugw4ZylvfRjfLe1XBSl4AaABAg  @yesjermplayzgamez511   \n",
       "3  UgxKwJLOpGXFfyFiuiJ4AaABAg      @RenuCacciatore75   \n",
       "4  UgwIUQ2Z7KlcW3NlCW94AaABAg           @Alexooooo11   \n",
       "\n",
       "                  author_id  \\\n",
       "0  UCJaSuHOdO58aWym_dnK8zMg   \n",
       "1  UCJaSuHOdO58aWym_dnK8zMg   \n",
       "2  UC89mwzaR6LHhk1pffPSBFdQ   \n",
       "3  UCYWEeTm05dumslGsMEAUErw   \n",
       "4  UCd4XgS8-vukv3F5JWx3eNPw   \n",
       "\n",
       "                                                text  like_count  \\\n",
       "0  don't worry about staying in japan pewds cas t...           0   \n",
       "1  9:15 I think it's the very first time pewds ha...           0   \n",
       "2  As a new dad, and someone who used to watch yo...           1   \n",
       "3  0:35 uuuu marimekko canvas bag i have one of t...           0   \n",
       "4                  He look so calculated for his age           0   \n",
       "\n",
       "           published_at            updated_at  \n",
       "0  2025-04-11T08:19:03Z  2025-04-11T08:19:03Z  \n",
       "1  2025-04-11T08:00:23Z  2025-04-11T08:00:23Z  \n",
       "2  2025-04-11T07:25:24Z  2025-04-11T07:25:24Z  \n",
       "3  2025-04-10T17:00:39Z  2025-04-10T17:08:10Z  \n",
       "4  2025-04-10T13:41:31Z  2025-04-10T13:41:31Z  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv from the youtube-sentiment-analysis\\data directory\n",
    "df = pd.read_csv('/youtube-sentiment-analysis/data/PewDiePie_We finally decided._comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c39547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations/comments of the video: 7886\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "# Print the number of observations\n",
    "print(f'Number of observations/comments of the video: {df.shape[0]}')\n",
    "print(f'Number of columns: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542ef7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 liked comments and the number of likes that they got:\n",
      "               author                                               text  \\\n",
      "0          @alien9279  These titles always make us think the worst, b...   \n",
      "1         @ElyjaJones  The fact that bjorn will be capable of speakin...   \n",
      "2     @ShankDeadPizza  I used to watch this man scream at barrels, no...   \n",
      "3           @xonedium  Bjorn is so curious, it's lovely to see. Happy...   \n",
      "4          @MuNky1022  Dad is speaking English and Swedish with him, ...   \n",
      "5            @つきい-h2t  I am Japanese. I am glad that you have decided...   \n",
      "6            @4-jax20                      12:52 \"OMG are you pewdiepie\"   \n",
      "7  @samuelstephen3416  This man will make a comeback as soon as his s...   \n",
      "8    @richherrera2866  The same guy who helped me cope with life and ...   \n",
      "9            @JKLunna  I love the fact that Pews is setting an exampl...   \n",
      "\n",
      "   like_count  \n",
      "0       60830  \n",
      "1       31662  \n",
      "2       17628  \n",
      "3       14046  \n",
      "4       11928  \n",
      "5       10631  \n",
      "6       10000  \n",
      "7        8045  \n",
      "8        6200  \n",
      "9        5213  \n"
     ]
    }
   ],
   "source": [
    "# Check who are the authors of the top 10 liked comments\n",
    "top_10 = df.sort_values(by = 'like_count', ascending=False).iloc[0:10,].reset_index(drop=True)\n",
    "print('Top 10 liked comments and the number of likes that they got:')\n",
    "print(top_10[['author', 'text', 'like_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look here\n",
    "# https://www.kirenz.com/blog/posts/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/\n",
    "\n",
    "# Here start the text preprocessing\n",
    "\n",
    "# Remove all the columns except the text column\n",
    "text_df = df[['text']].copy()\n",
    "# 1. Lowercase all the text\n",
    "text_df['text'] = text_df['text'].str.lower()\n",
    "\n",
    "# 2. Remove all the URLs\n",
    "text_df['text'] = text_df['text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c37c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaka1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# 3. Tokenize the text (we will use the nltk library for this)\n",
    "\n",
    "# 4. Remove all the stop words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add also a replace of the emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759123d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-sentiment-analysis",
   "language": "python",
   "name": "youtube-sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
